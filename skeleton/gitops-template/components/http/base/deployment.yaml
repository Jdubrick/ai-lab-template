apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:  
    tad.gitops.set/image: ".spec.template.spec.containers[0].image"
    tad.gitops.get/image: ".spec.template.spec.containers[0].image"
    tad.gitops.set/replicas: ".spec.replicas"
    tad.gitops.get/replicas: ".spec.replicas" 
  labels: 
    app.kubernetes.io/instance: ${{ values.name }}
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name:  ${{ values.name }}
    app.kubernetes.io/part-of: ${{ values.name }}  
  name: ${{ values.name }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance:  ${{ values.name }} 
  template:
    metadata: 
      labels:
        app.kubernetes.io/instance:  ${{ values.name }}
    spec:
      {%- if !values.vllmSelected %}
      initContainers:
      - name: model-file
        image: ${{ values.initContainer }}
        command: ['/usr/bin/install', "/model/model.file", "/shared/"]
        volumeMounts:
        - name: model-file
          mountPath: /shared
      {%- endif %}
      containers:
      - env:
        - name: MODEL_ENDPOINT
          value: http://0.0.0.0:${{ values.modelServicePort }}
        image:  ${{ values.appContainer }}
        name: app-inference
        ports:
        - containerPort: ${{ values.appPort }}
        securityContext:
          runAsNonRoot: true
      {%- if values.vllmSelected %}
      - image: ${{ values.vllmModelServiceContainer }}
        args: [
            "--model",
            "${{ values.vllmModelName }}",
            "--port",
            "${{ values.modelServicePort }}",
            "--download-dir",
            "/models-cache"]
        resources:
          limits:
            cpu: '8'
            memory: 24Gi
            nvidia.com/gpu: '1'
          requests:
            cpu: '6'
        volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: models-cache
          mountPath: /models-cache
      {%- else %}
      - env:
        - name: HOST
          value: "0.0.0.0"
        - name: PORT
          value: "${{ values.modelServicePort }}"
        - name: MODEL_PATH
          value: /model/model.file
        image: ${{ values.modelServiceContainer }}
        volumeMounts:
        - name: model-file
          mountPath: /model
      {%- endif %}
        name: app-model-service
        ports:
        - containerPort: ${{ values.modelServicePort }}
        securityContext:
          runAsNonRoot: true
      {%- if values.vllmSelected %}
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: "2Gi"
      - name: models-cache
        persistentVolumeClaim:
          claimName: vllm-models-cache

      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      {%- else %}
      volumes:
      - name: model-file
        emptyDir: {}
      {%- endif %}
